{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize models\n",
    "### Define constants\n",
    "Set the width and height of images that will be input to the models. Define the timesteps for the recurrent neural network. The timesteps is equal the number of images in the sequence of frames that you want the AI to analyze at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 96\n",
    "HEIGHT = 96\n",
    "TIMESTEPS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural network\n",
    "\n",
    "This project uses a simple architecture for the convolutional neural network with 3 convoltuions in order to optimize the model for speed. [This tutorial](https://developers.google.com/machine-learning/practica/image-classification) is a good hands on example of how convolutional neural networks work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = keras.models.Sequential()\n",
    "\n",
    "# First convolution\n",
    "cnn_model.add(keras.layers.Conv2D(16, 3, activation='relu', input_shape=(WIDTH, HEIGHT, 1)))\n",
    "cnn_model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "# Second convolution\n",
    "cnn_model.add(keras.layers.Conv2D(32, 3, activation='relu'))\n",
    "cnn_model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "# Third convolution\n",
    "cnn_model.add(keras.layers.Conv2D(64, 3, activation='relu'))\n",
    "cnn_model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "# Flaten so that we can pass the output to the recurrent neural network\n",
    "cnn_model.add(keras.layers.Flatten())\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural network\n",
    "The recurrent neural network used in this project is a simple [long short-term memory (LSTM)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) network. The [TimeDistributed](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed) layer allows us to connect the convolutional neural network to the recurrent neural network. The `TIMESTEPS` variable defines how many images the recurrent neural network will analyze at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.TimeDistributed(cnn_model, input_shape=(TIMESTEPS, WIDTH, HEIGHT, 1)))\n",
    "model.add(keras.layers.LSTM(256, return_sequences=False))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', keras.metrics.Recall(), keras.metrics.Precision()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure checkpoints\n",
    "We will use TensorFlow's [ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the weights of the model at each epoch so that we select the weights from the best epoch to use for the final model once training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_filepath = 'swing_checkpoints/cp-{epoch:04d}.ckpt'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=cp_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load annotations\n",
    "While collecting the data, I separated the data for pitches where I hit the ball, and pitches where I purposely didn't hit the ball, into different directories. Each pitch has it's own csv file associated with it. The csv file is formated like `filename,label`, where `filename` is the file name of an image for a single frame in the pitch. And `label` is 0 if a swing wasn't initiated during that frame, or 1 if a swing was initiated during that frame. The vast majority of data will be labeled 0, so we will separate the data to deal with some imbalanced data.\n",
    "### Load annotations for hits\n",
    "The data for hits will always end as soon as a swing is initiated, so the last image in each csv file will be for the frame where the swing was initiated, and therefore will be labeled as 1. I categorized data for hits into three differnent catagories:\n",
    "- Images where the ball is a significant distance away from the batter (more than two frames away from when the swing was initiated). This data falls into `csv_labels_other`\n",
    "- Images where the ball is one frame before the swing is initiated. This data falls into `csv_labels_hits_0`\n",
    "- Images where the swing is initiated `csv_labels_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CSV_HITS_DIR = '../data/examples/batting/hits/csvs_hits/'\nIMAGE_HITS_DIR = '../data/examples/batting/hits/ims_hits/'\ncsv_hits_paths = glob.glob(f'{CSV_HITS_DIR}*.csv')\n\ncsv_labels_other = []\ncsv_labels_strikes_0 = []\ncsv_labels_1 = []\n\n# read each csv file line by line\nfor csv_path in csv_hits_paths:\n    csv_data = []\n    with open(csv_path, newline='') as csv_file:\n        reader = csv.reader(csv_file, delimiter=',')\n        for row in reader:\n            csv_data.append(row)\n         \n    # if the number of images is less than the number of timesteps required then skip this pitch\n    if len(csv_data) < TIMESTEPS:\n        continue\n        \n    # create all possible time windows for the pitch\n    all_windows = []\n    for i in range(len(csv_data) - (TIMESTEPS - 1)):\n        window = [csv_data[i][0]]\n        for j in range(1, TIMESTEPS):\n            window.append(csv_data[i+j][0])\n        all_windows.append(window)\n    \n    # these are time windows where the ball is still far away from the batter\n    for i in range(len(all_windows) - 2):\n        csv_labels_other.append([0, all_windows[i], IMAGE_HITS_DIR])\n    \n    # this is the time window one frame before the swing is initiated\n    csv_labels_strikes_0.append([0, all_windows[-2], IMAGE_HITS_DIR])\n    \n    # this is the time window where the swing is initiated\n    csv_labels_1.append([1, all_windows[-1], IMAGE_HITS_DIR])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load annotations for balls\n",
    "Any pitches that were balls were pitches that I purposely didn't swing at. The data for balls ends approximately once the ball has passed the batter and is off the screen. I categorized data for balls into two differnent catagories:\n",
    "- Images where the ball is 3, 6, or 9 frames before the last frame. This data falls into `csv_labels_balls`. This data is meant to train the AI to recognize images of balls when they are in approximately similar positions to when the AI would need to initiate a swing\n",
    "- All other images fall into `csv_labels_other`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CSV_BALLS_DIR = '../data/examples/batting/balls/csvs_balls/'\nIMAGE_BALLS_DIR = '../data/examples/batting/balls/ims_balls/'\ncsv_balls_paths = glob.glob(f'{CSV_BALLS_DIR}*.csv')\n\ncsv_labels_balls = []\n\n# read each csv file line by line\nfor csv_path in csv_balls_paths:\n    csv_data = []\n    with open(csv_path, newline='') as csv_file:\n        reader = csv.reader(csv_file, delimiter=',')\n        for row in reader:\n            csv_data.append(row)\n    \n    # if the number of images is less than the number of timesteps required then skip this pitch\n    if len(csv_data) < 9 or len(csv_data) < TIMESTEPS:\n        continue\n    \n    # create all possible time windows for the pitch\n    all_windows = []\n    for i in range(len(csv_data) - (TIMESTEPS - 1)):\n        window = [csv_data[i][0]]\n        for j in range(1, TIMESTEPS):\n            window.append(csv_data[i+j][0])\n        all_windows.append(window)\n    \n    save_idxs = [3, 6, 9]\n    for i in range(len(all_windows) - 2):\n        if (len(all_windows) - i) in save_idxs:\n            csv_labels_balls.append([0, all_windows[i], IMAGE_BALLS_DIR])\n        csv_labels_other.append([0, all_windows[i], IMAGE_BALLS_DIR])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine annotations for hits and balls\n",
    "Combines all of the annotations for hits and balls and separates the annotations into training and validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.8\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# shuffle all of the annotations\n",
    "random.shuffle(csv_labels_other)\n",
    "random.shuffle(csv_labels_balls)\n",
    "random.shuffle(csv_labels_strikes_0)\n",
    "random.shuffle(csv_labels_1)\n",
    "\n",
    "# number of \"other\" images will be equivalent to number of images where a swing was initiated in order\n",
    "# to prevent a largely unbalanced dataset\n",
    "csv_labels_other = csv_labels_other[:len(csv_labels_1)]\n",
    "\n",
    "# combine all 0 labels together and split validation and training\n",
    "csv_labels_0_train = csv_labels_balls[:int(len(csv_labels_balls)*TRAIN_SPLIT)] + csv_labels_strikes_0[:int(len(csv_labels_strikes_0)*TRAIN_SPLIT)] + csv_labels_other[:int(len(csv_labels_other)*TRAIN_SPLIT)]\n",
    "csv_labels_0_val = csv_labels_balls[int(len(csv_labels_balls)*TRAIN_SPLIT):] + csv_labels_strikes_0[int(len(csv_labels_strikes_0)*TRAIN_SPLIT):] + csv_labels_other[int(len(csv_labels_other)*TRAIN_SPLIT):]\n",
    "\n",
    "# split validation and training for 1 labels\n",
    "csv_labels_1_train = csv_labels_1[:int(len(csv_labels_1)*TRAIN_SPLIT)]\n",
    "csv_labels_1_val = csv_labels_1[int(len(csv_labels_1)*TRAIN_SPLIT):]\n",
    "\n",
    "# combine all training and validation labels together and shuffle\n",
    "csv_labels_train = csv_labels_0_train + csv_labels_1_train\n",
    "csv_labels_val = csv_labels_0_val + csv_labels_1_val\n",
    "random.shuffle(csv_labels_train)\n",
    "random.shuffle(csv_labels_val)\n",
    "\n",
    "csv_labels = csv_labels_train + csv_labels_val\n",
    "print(len(csv_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "### Preprocess images\n",
    "Function to preprocess images. The process is the mask the image, convert the image to grayscale, resize the image, and then normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # convert from BGR to HSV image in order to apply color mask\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    masked_im = cv2.inRange(hsv, (0, 0, 0), (179, 57, 255))\n",
    "    masked_im = cv2.bitwise_and(image, image, mask=masked_im)\n",
    "    \n",
    "    # conver image to grayscale\n",
    "    masked_im = cv2.cvtColor(masked_im, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # resize image\n",
    "    masked_im = cv2.resize(masked_im, (WIDTH, HEIGHT))\n",
    "    \n",
    "    # normalize values to be between 0 and 1\n",
    "    masked_im = masked_im / 255\n",
    "    \n",
    "    masked_im = masked_im.reshape(WIDTH, HEIGHT, 1)\n",
    "    return masked_im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images\n",
    "Use the previously made annotations (csv_labels) to create the final data and labels set for the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "count = 0\n",
    "total = len(csv_labels)\n",
    "for label in csv_labels:\n",
    "    labels.append(label[0])\n",
    "    image_dir = label[2]\n",
    "    \n",
    "    window = []\n",
    "    for im_name in label[1]:\n",
    "        image = cv2.imread(f'{image_dir}{im_name}')\n",
    "        image = preprocess_image(image)\n",
    "        window.append(image)\n",
    "    data.append(window)\n",
    "    \n",
    "    count += 1\n",
    "    # print progress of loading images\n",
    "    print(f'{int((count/total)*100)}%', end='\\r', flush=True)\n",
    "    \n",
    "labels = np.array(labels)\n",
    "data = np.array(data)\n",
    "data = data.reshape(len(data), TIMESTEPS, WIDTH, HEIGHT, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Train the model on the data collected. You can adjust the `BATCH_SIZE` and `EPOCHS` paramaters as you like, however, these values worked best for me. The model does not need to train for very long before it will start to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "history = model.fit(\n",
    "    x=data,\n",
    "    y=labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=[model_checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "Evaluate the loss, accuracy, [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the model to determine the best checkpoint to load the weights from. The most important metrics here are precision and recall. Generally speaking, a model with low precision will swing at too many \"bad\" pitches. And a model with low recall will not swing at enough \"good\" pitches. Therefore, you should select a model that has decent and similar recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a list of loss results on training and validation data sets for each training epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "print(f'Checkpoint with lowest loss: {np.argmin(np.array(val_loss))}')\n",
    "\n",
    "# Retrieve a list of accuracy results on training and validation data sets for each training epoch\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "print(f'Checkpoint with highest accuracy: {np.argmax(np.array(val_acc))}')\n",
    "\n",
    "# Retrieve a list of recall results on training and validation data sets for each training epoch\n",
    "recall = history.history['recall']\n",
    "val_recall = history.history['val_recall']\n",
    "print(f'Checkpoint with highest recall: {np.argmax(np.array(val_recall))}')\n",
    "\n",
    "# Retrieve a list of precision results on training and validation data sets for each training epoch\n",
    "precision = history.history['precision']\n",
    "val_precision = history.history['val_precision']\n",
    "print(f'Checkpoint with highest precision: {np.argmax(np.array(val_precision))}')\n",
    "\n",
    "# Get range of epochs\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "plt.plot(epochs_range, acc, label='Training')\n",
    "plt.plot(epochs_range, val_acc, label='Validation')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot training and validation recall per epoch\n",
    "plt.plot(epochs_range, recall, label='Training')\n",
    "plt.plot(epochs_range, val_recall, label='Validation')\n",
    "plt.title('Training and validation recall')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot training and validation precision per epoch\n",
    "plt.plot(epochs_range, precision, label='Training')\n",
    "plt.plot(epochs_range, val_precision, label='Validation')\n",
    "plt.title('Training and validation precision')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "plt.plot(epochs_range, loss, label='Training')\n",
    "plt.plot(epochs_range, val_loss, label='Validation')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "Set the `best_cp` variable to the epoch that had the best metrics and save the model as an [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cp = 30\n",
    "best_cp_filepath = cp_filepath.format(epoch=best_cp)\n",
    "model.load_weights(best_cp_filepath)\n",
    "model.save('swing_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}